#Chapter 4

#Overview of the data
```{r, echo=FALSE}
# Loading the Boston dataset from the Mass package.
library(MASS)
library(ggplot2)
data("Boston")

summary(Boston)
str(Boston)

#This dataset has 506 observations and  14 variables. It contains information about Housing Values in the suburbs of Boston. 
#The dataframe  consists of the following columns: per capita crime rate by town (crim), proportion of residential land zoned for lots over 25,000 sq.ft (zn), proportion of non-retail business acres per town (indus), Charles River dummy variable (chas), nitrogen oxides concentration (nox), average number of rooms per dwelling (rm) proportion of owner-occupied units built prior to 1940 (age), weighted mean of distances to five Boston employment centres (dis), index of accessibility to radial highways (rad), full-value property-tax rate per $10,000 (tax), pupil-teacher ratio by town(ptratio), the proportion of blacks by town (black), lower status of the population % (lstat), median value of owner-occpupied homes in  $1000s  (medv).


#Visualization
library(MASS)
library(tidyr)
library(corrplot)
library(ggplot2)


# calculate the correlation matrix
cor_matrix <- cor(Boston) 

#round the matrix
cor_matrix2 <- round(cor_matrix,digits=2)

#plot matrix
corrplot(cor_matrix2, method="circle", type = 'upper')

# From this correlation matrix we can see e.g.  that the crime rate correlates quite strongly positively with rad, tax and even lstat. So accessibility to radial highways and the lower status of the population increase the crime rate, but so does the full-value property-tax rate. Variable Dis  has strong negative correlation with age, nox and indus instead. => So the weighted mean distance to the employment centers correlates negatively with the amount of pollution, porpotion of non-retail business acres and porpotion of units built before 1940. So further away from employment centers there is less pollution but also less business acres and so on.


```
#Standardizing the dataset and creating a test data set

```{r, echo=FALSE}

library(MASS)
data("Boston")

#  "Scaling is a technique for comparing data that isnâ€™t measured in the same way. It is normalization of the dataset. In the scaling we subtract the column means from the corresponding columns and divide the difference with standard deviation. When working with vectors or columns in a data frame, scaling is frequently employed." Source: https://www.r-bloggers.com/2021/12/how-to-use-the-scale-function-in-r/

# center and standardize variables
boston_scaled1 <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled1)

# class of the boston_scaled object
class(boston_scaled1)

# change the object to data frame and view data
boston_scaled <- as.data.frame(boston_scaled1)
View(boston_scaled)


#boston_scaled$crim <- as.numeric(boston_scaled$crim)

# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
quantile_names <- c("low", "med_low", "med_high", "high") 

crime <- cut(boston_scaled$crim, breaks = bins, label = quantile_names, include.lowest = TRUE)

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)


#CREATING A TRAIN AND A TEST SET
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```
#Linear discriminant analysis on the train set
```{r, echo=FALSE}
#Fitting the linear discriminant analysis on the train set and representing the results in the LDA (bi)plot
library(MASS)

# linear discriminant analysis
lda.fit = lda(crime ~ ., data=train)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch='classes') 

lda.arrows(lda.fit, myscale = 4)

```

##Predict LDA
```{r}
lda.fit = lda(crime ~ ., data=train)

# predict classes with test data (lda.pred was created with train data, now we want to see whether our model can predict the results for test data) 
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the result
table(correct = correct_classes, predicted = lda.pred$class)

# So our model can predict very well who belong to the  high-crime class (no false negatives, but one high was classified as med-high). The other classes instead have much more variation/the model is not performing as well in predicting them. 

```

#K-means clustering
```{r, echo=FALSE}
#Reloading the Boston dataset and standardizing it
library(MASS)
library(ggplot2)
data("Boston")
#"K-means might produce different results every time, because it randomly assigns the initial cluster centers. The function `set.seed()` can be used to deal with that."
set.seed(123)
boston_scaled <- as.data.frame(scale(Boston))

#Calculating the distances with euclidean distance matrix and manhattan distance matrix: 

# euclidean distance matrix and printing the summary
dist_eu <- dist(boston_scaled)
summary(dist_eu)

# manhattan distance matrix and pringing the summary
dist_man <- dist(boston_scaled, method = "manhattan")
summary(dist_man)

#Running K-means algorithm on the dataset
km <- kmeans(boston_scaled, centers = 2)
km

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)



#Determining the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering, centers=2 (number of clusters), because in the optimum number of cluster is the value of total WCSS changes radically (about 2 in the plot).
km <- kmeans(boston_scaled, centers = 2)
km


# plotting the dataset with clusters
pairs(boston_scaled, col = km$cluster)

```
# Chapter 2: Regression analysis

```{r}
#reading the data
date()
library(tidyverse)
students2014 <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/learning2014.txt", sep=",", header=TRUE)
#an option would be open the .csv file from the directory by activating this chunk: student2014 <- read_csv("./data/learning2014.csv")

```

## About the dataset
This dataset was originally created from data that is collected  from students attending the course "Johdatus yhteiskuntatieteeseen" in the fall of 2014. 183 of the 250 students attending the course participated in the study. Due to some missing data, there are only 166 subjects in the final dataset. The original data that this current dataset is based on can be found here: http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt


This current dataset has only 7 variables: gender, age, attitude, deep, stra, surf and points.  Combination variables in the original data (stra, deep, surf) have been scaled to the original scales by taking the mean. 

Attitude tells about students overall attitude towards statistics and has also been scaled back to the original scale by dividing it with the total number of questions.

Points tells the exam points. Observations with zero points have been removed, because these students did not attend the exam.

```{r}
# access the GGally and ggplot2 libraries
library(GGally)
library(ggplot2)
library(dplyr)

# create a more advanced plot matrix that shows the graphical overview of the data
p <- ggpairs(students2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p

#Summaries of the variables

summary.factor(students2014$gender)
summary(students2014$age)
summary(students2014$attitude)
summary(students2014$deep)
summary(students2014$stra)
summary(students2014$surf)
summary(students2014$points)


```

##Describtion of the variables

There are 110 females and 56 males. The mean age of the students is 25,5 years. The youngest student is 17 and oldeset one is 55 y.o. 

The mean attitude is 3.1 (range 1.4-5.0). 
The mean for deep learning is 3.7 (range 1.6-4.9), the mean for strategic learning is 3.1 (range 1.3-5.0) and the mean for surface learning is 2.8 (range 1.6-4.3).

The mean exam points was 22.7 (range 7.0-33.0). 

There was a low-moderate positive correlation between attitude and exam points in both genders. In males there was a low negative correlation between attitude and surface learning. In addition, there was a low-moderate negative correlation in males between deep learning and surface learning.

```{r}
#choose three variables as explanatory variables and fit a regression model where exam points is the target (dependent, outcome) variable. Show a summary of the fitted model 

# create a regression model with multiple explanatory variables
my_model <- lm(points ~ attitude + stra + surf, data = students2014)

# print out a summary of the model
summary(my_model)

#trying to find significant variables
my_model2 <- lm(points ~ attitude + stra, data = students2014)
summary(my_model2)

#still fixing the model
my_model3 <- lm(points ~ attitude, data = students2014)
summary(my_model3)
```




##Interpretation of the multiple regression  analysis

First I chose the three variables that were most likely to correlate with examination points. This judgment was made from the previous boxplot matrix (attitude, stra, surf). However, according to the model, only attitude turned out to be statsitically significantly associated with exam points (p-value < 0.001). For the other variables the p-values remained well over 0.05 and were rejected. The R-square value is  0.1951, meaning attitude explains only about 19 % of the variation in our target variable exam points.  

```{r}
#Producing the following diagnostic plots: Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage

plot(my_model3, which = 1) 
plot(my_model3, which = 2)
plot(my_model3, which = 5)
```



## Explaining the assumptions behind the  regression model (model3)
1) The data should be normally distributed. There are many ways to test/model this, but our previous histogram and Normal QQ-plot already tell us, that our residuals are fairly normally distributed, but there are some exceptions at the upper and lower ends.

2) One of the important assumptions is linear regression. It means that error remains constant along the values of the dependent variable.  We can check this with a residuals vs fitted plot. There the residuals are located randomly around 0-line,  which suggest that our assumption of the linear relationship is reasonable.

3) Another important assumptions of the linear model is homoscedasticity , which means that the variance of our residuals stay the same independent of the explanatory variable. We can check this with a residuals vs fitted plot. The residuals roughly form a horisontal band close to the 0-line which suggest that the variances of the error terms are equal. However, three residuals stand out and they might be outliers.

4) Outliers can be looked out by our Residuals vs Leverage model. Points that have high residual and high leverage at the same my be harmful to our model and compromise the accuracy of our regression. None of the points seem to be out of Cook's distance so we don't seem to have significant outliers after all.

After all our model seems to be reasonably well fitted.



